{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7WAGBd3EGh9",
        "outputId": "50637da6-b0f0-42ae-a054-89d90810ca61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#구글 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리 import\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# 데이터 준비\n",
        "import librosa\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import defaultdict\n",
        "\n",
        "# 시각화\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# 모델 생성\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional\n",
        "from keras.models import load_model"
      ],
      "metadata": {
        "id": "BURp058xFQcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU 사용 준비\n",
        "%tensorflow_version 2.x\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "id": "NX_BAkI1L12l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_path = \"/content/drive/MyDrive/Colab Notebooks/BD/\""
      ],
      "metadata": {
        "id": "XmhIEgDmV2p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. most common 데이터 선정"
      ],
      "metadata": {
        "id": "r96E0A73EJYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['1번 감정', '2번 감정', '3번 감정', '4번 감정', '5번 감정']\n",
        "\n",
        "df = pd.read_csv(f\"{main_path}/5차년도_2차.csv\", encoding='cp949')\n",
        "label = []\n",
        "\n",
        "for idx, data in df.iterrows():\n",
        "  df_em = {'Angry' : 0, 'Disgust' : 0, 'Fear' : 0, 'Happiness' : 0, 'Neutral' : 0, 'Sadness': 0, 'Surprise': 0}\n",
        "  for column in columns:\n",
        "      df_em[data[column].title()] += 1\n",
        "\n",
        "  # most common 감정 label\n",
        "  emotions = max(df_em, key = df_em.get)\n",
        "\n",
        "  label.append(emotions)\n",
        "\n",
        "wav_id = df['wav_id']\n",
        "speech = df['발화문']\n",
        "most_common = {'wav_id': wav_id, 'speech': speech, 'emotion' : label}\n",
        "most_common = pd.DataFrame(most_common)\n",
        "\n",
        "most_common.to_csv(f\"{main_path}/data/5차년도_2차_most_common.csv\", index = False)"
      ],
      "metadata": {
        "id": "mvyghvcUEN-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 분포 살피기 - 데이터 선정"
      ],
      "metadata": {
        "id": "JAaNdI7aEOV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_path_4 = f\"{main_path}most_common/4차년도_most_common.csv\" # 4차년도 데이터 경로\n",
        "df_path_5 = f\"{main_path}most_common/5차년도_most_common.csv\" # 5차년도 데이터 경로\n",
        "df_path_5_2 = f\"{main_path}most_common/5차년도_2차_most_common.csv\" # 5차년도 2차 데이터 경로"
      ],
      "metadata": {
        "id": "pFAjueRDER3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_4 = pd.read_csv(df_path_4)\n",
        "df_5 = pd.read_csv(df_path_5)\n",
        "df_5_2 = pd.read_csv(df_path_5_2)"
      ],
      "metadata": {
        "id": "PxWlje-xFpDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 분포 시각화\n",
        "order = [\"Angry\",\"Sadness\",\"Disgust\",\"Fear\",\"Happiness\",\"Surprise\", \"Neutral\"]\n",
        "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20,5))\n",
        "sns.countplot(x=\"emotion\",data=df_4,ax=axes[0],order=order).set_title(\"4th\")\n",
        "sns.countplot(x=\"emotion\",data=df_5,ax=axes[1],order=order).set_title(\"5th\")\n",
        "sns.countplot(x=\"emotion\",data=df_5_2, ax=axes[2],order=order).set_title(\"5th_2nd\")\n",
        "\n",
        "# 막대그래프에 숫자 출력\n",
        "for ax in axes:\n",
        "    for p in ax.patches:\n",
        "        ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                    ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),\n",
        "                    textcoords='offset points')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kT5j730AF2D4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Train Test 데이터셋 분류\n",
        " - 모든 감정이 150개씩 갖도록한다"
      ],
      "metadata": {
        "id": "yRZHtqULESZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 감정별 데이터 프레임추출\n",
        "angry_df = df_5_2[(df_5_2[\"emotion\"]==\"Angry\")]\n",
        "sadness_df = df_5_2[(df_5_2[\"emotion\"]==\"Sadness\")]\n",
        "disgust_df = df_5_2[(df_5_2[\"emotion\"]==\"Disgust\")]\n",
        "fear_df = df_5_2[(df_5_2[\"emotion\"]==\"Fear\")]\n",
        "happiness_df = df_5_2[(df_5_2[\"emotion\"]==\"Happiness\")]\n",
        "surprise_df = df_5_2[(df_5_2[\"emotion\"]==\"Surprise\")]\n",
        "neutral_df = df_5_2[(df_5_2[\"emotion\"]==\"Neutral\")]\n",
        "df_list = [angry_df, sadness_df, disgust_df, fear_df, happiness_df, surprise_df, neutral_df]"
      ],
      "metadata": {
        "id": "eKfB_42uGD-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_split(df):\n",
        "  # 랜덤하게 150개의 행을 선택하여 새로운 데이터프레임 생성\n",
        "  random_rows = df.sample(n=150, random_state=42)\n",
        "\n",
        "  # 선택된 행을 제외한 나머지 행으로 새로운 데이터프레임 생성\n",
        "  remaining_rows = df.drop(random_rows.index)\n",
        "  return random_rows, remaining_rows"
      ],
      "metadata": {
        "id": "9uu8XX7FGFu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 빈 데이터 프레임 생성\n",
        "train_df = pd.DataFrame()\n",
        "test_df = pd.DataFrame()\n",
        "for df in df_list:\n",
        "  # 각 감정 데이터 프레임별로 150개씩 추출\n",
        "  random_rows, remaining_rows = random_split(df)\n",
        "  # 추출된 데이터는 test데이터에 추가하고 나머지 데이터는 train 데이터프레임에 추가\n",
        "  train_df = pd.concat([train_df, remaining_rows])\n",
        "  test_df = pd.concat([test_df, random_rows])"
      ],
      "metadata": {
        "id": "PNOtMcGZGPTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 분포 시각화\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,5))\n",
        "sns.countplot(x=\"emotion\",data=train_df,ax=axes[0],order=order).set_title(\"Distribution of train\")\n",
        "sns.countplot(x=\"emotion\",data=test_df,ax=axes[1],order=order).set_title(\"Distribution of test\")\n",
        "\n",
        "for ax in axes:\n",
        "    for p in ax.patches:\n",
        "        ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                    ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),\n",
        "                    textcoords='offset points')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sOYHHpQfGctN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. 특성추출 - mfcc"
      ],
      "metadata": {
        "id": "B29HDnX4EitI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터셋 특성추출 - mfcc"
      ],
      "metadata": {
        "id": "fEaPtueIKRQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wav_path = f\"{main_path}data_5차2/\" # wav파일 경로"
      ],
      "metadata": {
        "id": "C9IG6s2BIzl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_pick(df):\n",
        "    mfcc_len_list=[]\n",
        "    mfcc_data = []\n",
        "    for name in tqdm(df['wav_id']):\n",
        "        audio_path = f\"{wav_path}{name}.wav\"\n",
        "        if(os.path.isfile(audio_path)):\n",
        "            y, sr = librosa.load(audio_path, sr=48000)\n",
        "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, hop_length=int(sr*0.01), n_fft=int(sr*0.02))\n",
        "            mfcc_data.append(mfccs.T)\n",
        "            mfcc_len_list.append(len(mfccs.T))\n",
        "        else:\n",
        "            # 빈 배열 반환\n",
        "            print(f\"Warning: File not found for {name}\")\n",
        "            return np.array([])\n",
        "    return mfcc_data, mfcc_len_list"
      ],
      "metadata": {
        "id": "K2cfOtk3HgGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_mfcc_data,train_mfcc_len = extract_features_pick(train_df)\n",
        "test_mfcc_data,test_mfcc_len = extract_features_pick(test_df)\n",
        "# ndarray로 변환\n",
        "train_mfcc_data_np = np.array(train_mfcc_data, dtype=object)\n",
        "test_mfcc_data_np = np.array(test_mfcc_data, dtype=object)"
      ],
      "metadata": {
        "id": "3MYjLpalJWHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 둘 길이확인\n",
        "print(f'{max(train_mfcc_len)}')\n",
        "print(f'{max(test_mfcc_len)}')"
      ],
      "metadata": {
        "id": "70cSG-XuJpcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 레이블처리 - 원 핫 인코딩"
      ],
      "metadata": {
        "id": "CSZlb5IZKUXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = train_df[\"emotion\"]\n",
        "test_labels = test_df[\"emotion\"]"
      ],
      "metadata": {
        "id": "Q8-tl3MsJo0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 원핫인코딩\n",
        "train_labels_encoding = pd.get_dummies(train_labels).values\n",
        "test_labels_encoding = pd.get_dummies(test_labels).values\n",
        "\n",
        "# encoding 저장\n",
        "np.save(\"train_labels_encoding_new\", train_labels_encoding)\n",
        "np.save(\"test_labels_encoding_new\", test_labels_encoding)"
      ],
      "metadata": {
        "id": "79hILLRtKct6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. 모델생성 (LSTM)"
      ],
      "metadata": {
        "id": "E5s2JYS6EfJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lstm(X_train):\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    # LSTM 모델 구축\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Bidirectional(LSTM(128, return_sequences=False)))\n",
        "    model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "    # 모델 컴파일\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "BoX47_86LwMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. 데이터 증가\n",
        "- pitch shift를 진행"
      ],
      "metadata": {
        "id": "1Vrn9Qc9EvT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 가상의 데이터셋 예시\n",
        "sentiments = ['Sadness', 'Angry', 'Fear', 'Neutral', 'Disgust', 'Happiness', 'Suprise']\n",
        "data_per_sentiment = {'Angry': 2169, 'Sadness': 3290, 'Disgust': 1466, 'Fear': 1018, 'Happiness': 3900, 'Suprise': 693, 'Neutral': 5788}\n",
        "index_per_sentiment = {'Angry': 2168, 'Sadness': 5458, 'Disgust': 6924, 'Fear': 7942, 'Happiness': 11842, 'Suprise': 12535, 'Neutral': 18323}\n",
        "\n",
        "# 필요한 최소 데이터 개수\n",
        "target_data_count = 5788\n",
        "\n",
        "# 각 클래스별로 필요한 augmentation 횟수\n",
        "augmentation_counts = defaultdict(int)\n",
        "\n",
        "for sentiment in sentiments:\n",
        "    current_count = data_per_sentiment[sentiment]\n",
        "    remaining_count = max(0, target_data_count - current_count)\n",
        "    augmentation_counts[sentiment] = remaining_count\n",
        "\n",
        "# 각 클래스별로 필요한 augmentation 횟수 출력\n",
        "print(\"Augmentation Counts:\")\n",
        "for sentiment, count in augmentation_counts.items():\n",
        "    print(f\"{sentiment}: {count}\")"
      ],
      "metadata": {
        "id": "QUnWSRp2MTi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# augmentation 함수 정의\n",
        "def pitch_shift_mfcc(mfcc, sr, semitone_shift):\n",
        "    y_pitch_shifted = librosa.effects.pitch_shift(mfcc, sr, n_steps=semitone_shift)\n",
        "    mfcc_pitch_shifted = librosa.feature.mfcc(y_pitch_shifted, sr=sr, n_mfcc=13)\n",
        "    return mfcc_pitch_shifted"
      ],
      "metadata": {
        "id": "p0GvE-fKMqim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_mfcc_list = []\n",
        "augmented_y_label_list = []\n",
        "\n",
        "# 각 클래스별로 augmentation 적용\n",
        "for sentiment in tqdm(sentiments):\n",
        "    current_count = data_per_sentiment[sentiment]\n",
        "    remaining_count = augmentation_counts[sentiment]\n",
        "    index = index_per_sentiment[sentiment]\n",
        "\n",
        "    # 현재 데이터 수가 필요한 데이터 수보다 작은 경우에만 augmentation 수행\n",
        "    while remaining_count > 0:\n",
        "\n",
        "        # 랜덤하게 MFCC 선택\n",
        "        selected_index = np.random.randint(index-current_count+1, index)\n",
        "        selected_mfcc = train_mfcc_data_np[selected_index]\n",
        "        selected_y_label = train_labels_encoding[selected_index]\n",
        "\n",
        "        # pitch shifting을 사용한 augmentation\n",
        "        semitone_shift = np.random.uniform(low=-2, high=2)  # 랜덤한 음정 변경\n",
        "        augmented_mfcc = librosa.effects.pitch_shift(selected_mfcc, sr=22050, n_steps=semitone_shift)\n",
        "        # MFCC를 저장\n",
        "        augmented_mfcc_list.append(augmented_mfcc)\n",
        "        augmented_y_label_list.append(selected_y_label)\n",
        "\n",
        "        remaining_count -= 1\n",
        "\n",
        "        if remaining_count <= 0:\n",
        "            break\n",
        "\n",
        "# 리스트에서 NumPy 배열로 변환\n",
        "augmented_mfcc_np = np.array(augmented_mfcc_list, dtype=object)"
      ],
      "metadata": {
        "id": "tdjEvAqGMs2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 증가 데이터 저장\n",
        "np.save(\"augmented_mfcc_np\",augmented_mfcc_np)\n",
        "np.save(\"augmented_mfcc_np_label\",augmented_y_label_list)"
      ],
      "metadata": {
        "id": "Yb2uydQaN6WP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 기존 데이터와 증가 데이터 병합\n",
        "final_X_train_np = np.concatenate((train_mfcc_data_np ,augmented_mfcc_np), axis=0)\n",
        "final_y_one_hot_np = np.concatenate((train_labels_encoding ,augmented_y_label_list), axis=0)\n",
        "\n",
        "# 레이블 개수확인\n",
        "augmented_label_argmax = np.argmax(final_y_one_hot_np,axis=1)\n",
        "unique_values, counts = np.unique(augmented_label_argmax, return_counts=True)\n",
        "\n",
        "for value, count in zip(unique_values, counts):\n",
        "    print(f\"{value}: {count} times\")"
      ],
      "metadata": {
        "id": "HvkffLDgODZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#증가한 특성 데이터 + 원본데이터 저장\n",
        "np.save('final_X_train',final_X_train_np)\n",
        "#증가한 레이블 + 원본 레이블 저장\n",
        "np.save('final_y_one_hot',final_y_one_hot_np)"
      ],
      "metadata": {
        "id": "JtaNTED_O68E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. 훈련 및 예측\n",
        "- 데이터 증가 전 후를 나누어 진행한다"
      ],
      "metadata": {
        "id": "NFtFoC_xExHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가 함수\n",
        "def evaluate_model(model, test_features_padded, test_label):\n",
        "  test_loss, test_acc = model.evaluate(test_features_padded, test_label)\n",
        "  print(\"Test Loss:\", test_loss)\n",
        "  print(\"Test Accuracy:\", test_acc)"
      ],
      "metadata": {
        "id": "yhWNm2ayPiXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#정답 오답 개수 카운팅\n",
        "def count_correct_incorrect(model, test_label):\n",
        "  predictions = model.predict(test_mfcc_padded)\n",
        "  predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "  answer_labels = np.argmax(test_label, axis=1)\n",
        "\n",
        "  correct = []\n",
        "  incorrect_answer = []\n",
        "  incorrect_pred = []\n",
        "\n",
        "  for predict, answer in zip(predicted_labels,answer_labels):\n",
        "    if predict == answer:\n",
        "      correct.append(predict)\n",
        "    else:\n",
        "      incorrect_answer.append(answer)\n",
        "      incorrect_pred.append(predict)\n",
        "\n",
        "  correct_counts = Counter(correct)\n",
        "\n",
        "  incorrect_answer_counts = Counter(incorrect_answer)\n",
        "\n",
        "  print(\"Correct Counts:\", correct_counts)\n",
        "  print(\"Incorrect Answer Counts:\", incorrect_answer_counts)\n",
        "  return correct_counts"
      ],
      "metadata": {
        "id": "AxqdjUJGQHBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 증가 전 데이터 예측"
      ],
      "metadata": {
        "id": "_4X9KHE7SWV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 패딩\n",
        "train_mfcc_padded = pad_sequences(train_mfcc_data_np,maxlen=5897, dtype='float32')\n",
        "test_mfcc_padded = pad_sequences(test_mfcc_data_np,maxlen=5897, dtype='float32')"
      ],
      "metadata": {
        "id": "M-fBw258LH85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_mfcc_padded = pad_sequences(train_mfcc_padded,maxlen=5897, dtype='float32')\n",
        "\n",
        "# train validation 나누기\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_mfcc_padded, train_labels_encoding, test_size=0.2, random_state=42)\n",
        "\n",
        "# 모델 훈련\n",
        "model = build_lstm(X_train)\n",
        "with tf.device('/device:GPU:0'):\n",
        "  model.fit(X_train, y_train, epochs=40, batch_size=16, validation_data=(X_val, y_val))\n",
        "\n",
        "#모델 저장\n",
        "  model.save(f\"{main_path}models/lstm_epoch40_basic.h5\")\n",
        "\n",
        "#평가\n",
        "evaluate_model(model,test_mfcc_padded,test_labels_encoding)"
      ],
      "metadata": {
        "id": "r3khJ7a4SRKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basic_correct = count_correct_incorrect(model,test_labels_encoding)"
      ],
      "metadata": {
        "id": "lCfWufNxQiOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(test_mfcc_padded)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "answer_labels = np.argmax(test_labels_encoding, axis=1)\n",
        "comparison = pd.DataFrame()\n",
        "\n",
        "# 정답 틀린것들만 수집\n",
        "indices = [i for i, (pred, ans) in enumerate(zip(predicted_labels, answer_labels)) if pred != ans]\n",
        "\n",
        "# 예측 정답 데이터 프레임\n",
        "comparison['Predicted'] = [predicted_labels[i] for i in indices]\n",
        "comparison['Answer'] = [answer_labels[i] for i in indices]\n",
        "\n",
        "#시각화\n",
        "unique_values, counts = np.unique(predicted_labels, return_counts=True)\n",
        "\n",
        "# 그래프 그리기\n",
        "plt.bar(unique_values, counts, alpha=0.7)\n",
        "plt.xlabel('Predicted Value')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Predicted Values before argumentation')\n",
        "\n",
        "# 숫자 표시하기\n",
        "for i, count in enumerate(counts):\n",
        "    plt.text(unique_values[i], count + 0.1, str(count), ha='center', va='bottom')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HTOCbD3cQlTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측 비율 시각화\n",
        "grouped_data = comparison.groupby('Answer')['Predicted'].value_counts().unstack(fill_value=0)\n",
        "\n",
        "grouped_data.plot(kind='bar', stacked=True)\n",
        "plt.xlabel('Answer Label')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Count of Predicted Labels for Each Answer Label before argumentation')\n",
        "plt.legend(title='Predicted Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7h_ctO5HRHAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 증가 후 데이터 예측"
      ],
      "metadata": {
        "id": "unxAIc4eRdqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 증가 후 데이터 패딩\n",
        "train_mfcc_padded_arg = pad_sequences(final_X_train_np,maxlen=5897, dtype='float32')\n",
        "\n",
        "# train validation 나누기\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_mfcc_padded_arg, final_y_one_hot_np, test_size=0.2, random_state=42)\n",
        "\n",
        "# 모델 훈련\n",
        "model = build_lstm(X_train)\n",
        "with tf.device('/device:GPU:0'):\n",
        "  model.fit(X_train, y_train, epochs=40, batch_size=16, validation_data=(X_val, y_val))\n",
        "#모델 저장\n",
        "  model.save(f\"{main_path}models/lstm_argu.h5\")\n",
        "#평가\n",
        "evaluate_model(model,test_mfcc_padded,test_labels_encoding)"
      ],
      "metadata": {
        "id": "sgEzNud1RYDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "argu_correct = count_correct_incorrect(model,test_labels_encoding)"
      ],
      "metadata": {
        "id": "MDFTI4kiWO-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(test_mfcc_padded)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "answer_labels = np.argmax(test_labels_encoding, axis=1)\n",
        "comparison = pd.DataFrame()\n",
        "\n",
        "# 정답 틀린것들만 수집\n",
        "indices = [i for i, (pred, ans) in enumerate(zip(predicted_labels, answer_labels)) if pred != ans]\n",
        "\n",
        "# 예측 정답 데이터 프레임\n",
        "comparison['Predicted'] = [predicted_labels[i] for i in indices]\n",
        "comparison['Answer'] = [answer_labels[i] for i in indices]\n",
        "\n",
        "#시각화\n",
        "unique_values, counts = np.unique(predicted_labels, return_counts=True)\n",
        "\n",
        "# Plotting a bar plot\n",
        "plt.bar(unique_values, counts, alpha=0.7)\n",
        "plt.xlabel('Predicted Value')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Predicted Values after argumentation')\n",
        "\n",
        "# Adding annotations\n",
        "for i, count in enumerate(counts):\n",
        "    plt.text(unique_values[i], count + 0.1, str(count), ha='center', va='bottom')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b3PWPti9WQ71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측 비율 시각화\n",
        "grouped_data = comparison.groupby('Answer')['Predicted'].value_counts().unstack(fill_value=0)\n",
        "\n",
        "# Plotting\n",
        "grouped_data.plot(kind='bar', stacked=True)\n",
        "plt.xlabel('Answer Label')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Count of Predicted Labels for Each Answer Label after argumentation')\n",
        "plt.legend(title='Predicted Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sI7sgPa_Xzjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. 결과정리"
      ],
      "metadata": {
        "id": "HiLMNG3qW23U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = basic_correct\n",
        "\n",
        "# 키(key)와 값(value) 추출\n",
        "keys = list(data.keys())\n",
        "values = list(data.values())\n",
        "\n",
        "# 막대 곡선 그래프 생성\n",
        "sns.lineplot(x=keys, y=values, marker='o', label='Line Plot')\n",
        "sns.barplot(x=keys, y=values, alpha=0.7, color='skyblue', label='Bar Plot')\n",
        "\n",
        "\n",
        "# 라벨 및 타이틀 추가\n",
        "plt.xlabel('Key')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Distribution of Correct (before Argumentaion)')\n",
        "\n",
        "\n",
        "# 그래프 표시\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Lbvk669OW0_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = argu_correct\n",
        "\n",
        "# 키(key)와 값(value) 추출\n",
        "keys = list(data.keys())\n",
        "values = list(data.values())\n",
        "\n",
        "# 막대 곡선 그래프 생성\n",
        "sns.lineplot(x=keys, y=values, marker='o', label='Line Plot')\n",
        "sns.barplot(x=keys, y=values, alpha=0.7, color='skyblue', label='Bar Plot')\n",
        "\n",
        "\n",
        "# 라벨 및 타이틀 추가\n",
        "plt.xlabel('Key')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Distribution of Correct (after Argumentaion)')\n",
        "\n",
        "\n",
        "# 그래프 표시\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_66RsEAhXpCw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}